{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b02da6f2585e11",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from utils.helper_clustering_functions import KMeanClustering ,kmeans_with_smape_ts ,kmeans_with_min_distance\n",
    "from utils.helper_similarity_metrics import calculate_dtw_distance , calculate_error_metrics ,calculate_cosine_similarity_char ,CoinCrossMappingSimilarity ,smape,smape_distance_metric\n",
    "from utils.helper_visualization_functions import plot_and_save , cluster_visualization_of_time_series\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import csv\n",
    "import os \n",
    "import glob\n",
    "import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521cfe51-87a1-4165-9b49-30b9051bfbf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:25:30.300911Z",
     "start_time": "2024-10-18T10:25:30.293229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Directories : ProcessedData\n",
      "Creating Directories : VisualizationData\n",
      "Creating Directories : TestingGarbage\n",
      "Creating Directories : SimilarityResults\n",
      "Creating Directories : ClusterResultsVisualization\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "directory_names = {\n",
    "    \"preprocessed_data_dir_name\":\"ProcessedData\",\n",
    "    \"visualization_data_dir_name\":\"VisualizationData\",\n",
    "    \"testing_garbage_dir_name\" :\"TestingGarbage\",\n",
    "    \"ResultsDirectory\":\"SimilarityResults\",\n",
    "    \"cluster_dir_path\":\"ClusterResultsVisualization\"\n",
    "}\n",
    "for key , value in directory_names.items():\n",
    "    print(f\"Creating Directories : {value}\")\n",
    "    os.makedirs(directory_names[key], exist_ok=True)\n",
    "    \n",
    "files_path = {\n",
    "    'raw_price_data' : os.path.join(\"Datasets\",\"raw_datasets\",\"prices.csv\"),\n",
    "    'raw_token_names' : os.path.join(\"Datasets\",\"raw_datasets\",\"token_names.csv\"),\n",
    "    \"token_names\":os.path.join(\"TestingGarbage\",\"token_names.csv\"),\n",
    "    \"similarity_results_file_path\":os.path.join(\"SimilarityResults\",\"similarity_results_version_0.1.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ae2aa-ba8f-4276-9f56-f5fdd70b4934",
   "metadata": {},
   "source": [
    "# Helping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95408a9323b6ea11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:25:38.657695Z",
     "start_time": "2024-10-18T10:25:38.640794Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_pair(token1, token2, price_pivot_df, directory_names, results_file):\n",
    "    try:\n",
    "        token_folder_name = os.path.join(directory_names['visualization_data_dir_name'], token1)\n",
    "        os.makedirs(token_folder_name, exist_ok=True)\n",
    "\n",
    "        plot_filename = f\"{token1}_vs_{token2}_dtw.png\"\n",
    "        saving_file_path = os.path.join(token_folder_name, plot_filename)\n",
    "\n",
    "        # Check if the plot already exists\n",
    "        if os.path.exists(saving_file_path):\n",
    "            return\n",
    "\n",
    "        # Drop missing values and find common indices\n",
    "        ts1 = price_pivot_df[token1].dropna()\n",
    "        ts2 = price_pivot_df[token2].dropna()\n",
    "        common_index = ts1.index.intersection(ts2.index)\n",
    "        ts1_common = ts1.loc[common_index]\n",
    "        ts2_common = ts2.loc[common_index]\n",
    "\n",
    "        # Proceed if common data points exist\n",
    "        if len(ts1_common) > 0 and len(ts2_common) > 0:\n",
    "            # Calculate similarity metrics\n",
    "            dtw_distance = calculate_dtw_distance(ts1_common, ts2_common)\n",
    "            cosine_sim = calculate_cosine_similarity_char(token1, token2)\n",
    "\n",
    "            # Calculate error metrics\n",
    "            mae, rmse, mape, smape = calculate_error_metrics(ts1_common, ts2_common)\n",
    "            similarity_score = {\n",
    "                \"mae\": mae,\n",
    "                \"rmse\": rmse,\n",
    "                \"mape\": mape,\n",
    "                \"smape\": smape,\n",
    "                \"dtw\": dtw_distance,\n",
    "                \"cosine_sim\": cosine_sim\n",
    "            }\n",
    "\n",
    "            # Save plot only if SMAPE is above 15\n",
    "            if smape < 15:\n",
    "                plot_and_save(ts1=ts1_common, \n",
    "                              ts2=ts2_common, \n",
    "                              token1=token1, \n",
    "                              token2=token2, \n",
    "                              similarity_score=similarity_score, \n",
    "                              saving_file_path=saving_file_path)\n",
    "\n",
    "            # Save the results in a CSV file\n",
    "            with open(results_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([token1, token2, dtw_distance, mae, rmse, mape, smape, cosine_sim])\n",
    "\n",
    "        # Free memory if necessary\n",
    "        del ts1_common, ts2_common\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tokens {token1} and {token2}: {e}\")\n",
    "\n",
    "def CoinCrossMappingSimilarity_multiprocessing(results_with_cluster_id=None, price_pivot_df=None, files_path=None, directory_names=None,max_workers=2):\n",
    "    results_file = files_path['similarity_results_file_path']\n",
    "\n",
    "    if not os.path.exists(results_file):\n",
    "        with open(results_file, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Token1', 'Token2', 'DTW_Distance', 'MAE', 'RMSE', 'MAPE', 'SMAPE', \"cosine_sim\"])\n",
    "\n",
    "    # Iterate through each cluster\n",
    "    for cluster_id, cluster_group in results_with_cluster_id.groupby('cluster'):\n",
    "        print(f\"Processing Cluster ID: {cluster_id}\")\n",
    "        tokens = list(cluster_group['token_id'].unique())\n",
    "        print(f\"Number of tokens in this cluster: {len(tokens)}\")\n",
    "\n",
    "        # Create a pool of processes\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for i, token1 in tqdm(enumerate(tokens), total=len(tokens), desc=f'Cluster {cluster_id}'):\n",
    "                for j, token2 in enumerate(tokens):\n",
    "                    if i < j:  # Skip redundant calculations\n",
    "                        futures.append(executor.submit(process_pair, token1, token2, price_pivot_df, directory_names, results_file))\n",
    "\n",
    "            # Wait for all processes to complete\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Token Pairs\"):\n",
    "                future.result()\n",
    "\n",
    "    gc.collect()\n",
    "    total_detected_similar_tokens = len(glob.glob(f\"{directory_names['visualization_data_dir_name']}/*/*.png\"))\n",
    "    return total_detected_similar_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c61cd-ad58-4c73-a916-146efb50ae7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:29:11.350865Z",
     "start_time": "2024-10-18T10:25:40.286221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of price data : (22021830, 4)\n",
      "shap of token names data : (1000, 4)\n",
      "Number of Unique Token : 1000\n",
      "Number of Unique Token after merging : 541\n",
      "Cluster ID: -1\n",
      "Cluster id : [-1]\n",
      "Number of tokens in this cluster: 5\n",
      "file saved at path : ClusterResultsVisualization/cluster_-1.png\n",
      "Cluster ID: 0\n",
      "Cluster id : [0]\n",
      "Number of tokens in this cluster: 15\n",
      "file saved at path : ClusterResultsVisualization/cluster_0.png\n",
      "Processing Cluster ID: -1\n",
      "Number of tokens in this cluster: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cluster -1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 87.17it/s]\n",
      "Processing Token Pairs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Cluster ID: 0\n",
      "Number of tokens in this cluster: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cluster 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 327.18it/s]\n",
      "Processing Token Pairs:  58%|███████████████████████████████████████████████████████████████████████████████████████▋                                                               | 61/105 [01:35<01:13,  1.66s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # step-1 Reading price data \n",
    "    cols_to_ignore = ['Unnamed: 0']\n",
    "    raw_price_df = pd.read_csv(\n",
    "                        files_path['raw_price_data'],\n",
    "                        compression='gzip',\n",
    "                        usecols=lambda col: col not in cols_to_ignore)\n",
    "    \n",
    "    # raw_price_df =  raw_price_df.head(1000*20)\n",
    "    # step-2 Reading Token data\n",
    "    token_names_df = pd.read_csv(files_path['raw_token_names'] )\n",
    "\n",
    "    # token_names_df = token_names_df.head(1000)\n",
    "    number_of_unique_token =  len(token_names_df['id'].unique())\n",
    "    \n",
    "    print(f\"Shape of price data : {raw_price_df.shape}\")\n",
    "    print(f\"shap of token names data : {token_names_df.shape}\")\n",
    "    print(f\"Number of Unique Token : {number_of_unique_token}\")\n",
    "\n",
    "    # step-3 Merging the data\n",
    "    merged_df = raw_price_df.merge(token_names_df, left_on=['network_id', 'base_currency'], right_on=['network_id', 'id'])\n",
    "    # merged_df = raw_price_df.merge(token_names_df, left_on=['base_currency'], right_on=['id'])\n",
    "    \n",
    "    number_of_unique_token_after_merging =  len(merged_df['base_currency'].unique())\n",
    "    \n",
    "    print(f\"Number of Unique Token after merging : {number_of_unique_token_after_merging}\")\n",
    "\n",
    "    merged_df['token_id'] = merged_df['base_currency'].astype(str) + '_' + merged_df['network_id'].astype(str)\n",
    "\n",
    "    # selective_base_currency = list(merged_df['base_currency'].unique())[:20]\n",
    "    # merged_df = merged_df[ merged_df['base_currency'].isin(selective_base_currency)  ]\n",
    "\n",
    "    price_pivot = merged_df.pivot_table(index='timestamp_utc', columns='token_id', values='open')\n",
    "    price_pivot = price_pivot.dropna()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    price_scaled = scaler.fit_transform(price_pivot.T)  # Transpose so each row is a token\n",
    "\n",
    "\n",
    "    # labels = KMeanClustering(n_clusters=10,\n",
    "    #                 price_scaled=price_scaled,\n",
    "    #                metric = \"dtw\",\n",
    "    #                 max_iter=50)\n",
    "    \n",
    "    # cluster_results = pd.DataFrame({'token_id': price_pivot.columns, 'cluster': labels})\n",
    "\n",
    "    # results_with_cluster_id = pd.merge(merged_df_filtered,cluster_results,on='token_id')\n",
    "\n",
    "    # results_with_cluster_id['timestamp_utc'] = pd.to_datetime(results_with_cluster_id['timestamp_utc'])\n",
    "    # results_with_cluster_id = results_with_cluster_id.sort_values(by='timestamp_utc')\n",
    "\n",
    "    # cluster_visualization_of_time_series(results_with_cluster_id=results_with_cluster_id,cluster_dir_path = directory_names['cluster_dir_path'])\n",
    "\n",
    "\n",
    "        \n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=100, min_samples=3, metric=smape_distance_metric)\n",
    "    \n",
    "    labels = dbscan.fit_predict(price_pivot.T.values)\n",
    "    \n",
    "    cluster_results = pd.DataFrame({'token_id': price_pivot.columns, 'cluster': labels})\n",
    "    \n",
    "    results_with_cluster_id = pd.merge(merged_df,cluster_results,on='token_id')\n",
    "    \n",
    "    results_with_cluster_id['timestamp_utc'] = pd.to_datetime(results_with_cluster_id['timestamp_utc'])\n",
    "    results_with_cluster_id = results_with_cluster_id.sort_values(by='timestamp_utc')\n",
    "    \n",
    "    cluster_visualization_of_time_series(results_with_cluster_id=results_with_cluster_id,cluster_dir_path = directory_names['cluster_dir_path'])\n",
    "    \n",
    "    price_pivot_df = results_with_cluster_id.pivot_table(index='timestamp_utc', columns='token_id', values='open')\n",
    "\n",
    "\n",
    "    total_similar_tokens = CoinCrossMappingSimilarity_multiprocessing(results_with_cluster_id, price_pivot_df, files_path, directory_names, max_workers=2)\n",
    "    print(f\"Total detected similar tokens: {total_similar_tokens}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823fa88-b7f6-48f0-b4a9-44ced8fb97d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcae2bfe-50a1-4ac7-b9d9-eea45ef8475e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745a03e-89ab-45e4-93e6-06e5684410a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
